---
title: "SIM. Assignment 2: Telco Customer Churn"
author: "Adrià Casanova, Víctor Garcia, Zhengyong Ji"
date: "2024-01-05"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: no
    header-includes:
      - \pagenumbering{arabic}
editor_options:
  chunk_output_type: console
---

- Profiling: AN EXTRA POINT TO BE DONE. WE CAN DO IT LATTER. USE THE SCRIPT PROVIDED BY MVA ???
- SI GENEREM UN WORD, L'HEM DE JUSTIFICAR I HEM DE NUMERAR-NE LES PÀGINES ABANS DE CONVERTIR-LO A PDF.
- #Pàgines actual (sense annex): 41/50

# 0. Introduction

In this project, we will study the data set "Telco Customer Churn", which can be found at https://www.kaggle.com/datasets/blastchar/telco-customer-churn. Our goal is to analyze the correlation between the amount of customers who left within the last month (Churn) and different features that describe the customer and the services he/she/they has signed up for. Then, we will build a logistic model that will allow us to predict the variable Churn.

All members have contributed equally to all parts of the project.

```{r Clean workspace. Load libraries, echo=F, results='hide'}
if(!is.null(dev.list())) dev.off()
rm(list = ls())

library(dplyr) 
library(car)
library(DataExplorer)
library(FactoMineR)
library(caTools)
library(chemometrics)
library(corrplot)
source("profiling_func.R")
library(ROSE)
```

```{r Import dataset, echo=F, results='hide'}
df = read.csv("WA_Fn-UseC_-Telco-Customer-Churn.xls",header=T, sep=",",
              stringsAsFactors=TRUE)

head(df)
```

We start by taking a first general look at the dataset.

```{r First look at df}
str(df)
summary(df)
```

The data set contains `r dim(df)[1]` observations of `r dim(df)[2]` variables.

\newpage
# 1. Data preparation

The first part of the project consisted on doing some basic data preparation to ensure that data is ready for the next sections.

Firstly, we checked that all datatypes were consistent with the metadata and declared "SeniorCitizen" as a factor, as it represented a qualitative concept.

```{r SeniorCitizen to categorical}
df$SeniorCitizen <- factor(df$SeniorCitizen, labels = c("Yes", "No"))
```

Secondly, we discretized all numeric variables by splitting data into 4 categories. Their boundaries were obtained simply by dividing the total range in 4 equal intervals and the distribution was checked using histograms to ensure that they were similar to the original variables.

```{r Discretize tenure}
df$c.tenure <- df$tenure # Create a new variable called Categorical.tenure
m.tenure <- max(df$tenure, na.rm = TRUE)
df$c.tenure <- replace(df$c.tenure, df$tenure <= m.tenure/4, m.tenure/4)
for (i in 1:3) {
  idx <- (m.tenure*i/4 < df$tenure) & (df$tenure <= m.tenure*(i+1)/4)
  df$c.tenure <- replace(df$c.tenure, idx, m.tenure*(i+1)/4)
}
min(df$tenure, na.rm = TRUE)
breakpts <- seq(m.tenure/4, m.tenure, m.tenure/4); breakpts
df$c.tenure <- factor(df$c.tenure, labels = c("(-1,18]", "(18,36]",
                                              "(36,54]", "(54,72]"))
summary(df$c.tenure)
par(mfrow=c(1,2))
plot(df$c.tenure, main = "Barplot of df$c.tenure")
hist(df$tenure)
```

```{r Discretize TotalCharges}
df$c.TotalCharges <- df$TotalCharges
m.TotalCharges <- max(df$TotalCharges, na.rm = TRUE)
df$c.TotalCharges <- replace(df$c.TotalCharges, df$TotalCharges <= m.TotalCharges/4, m.TotalCharges/4)
for (i in 1:3) {
  idx <- (m.TotalCharges*i/4 < df$TotalCharges) & (df$TotalCharges <=
                                                     m.TotalCharges*(i+1)/4)
  df$c.TotalCharges <- replace(df$c.TotalCharges, idx, m.TotalCharges*(i+1)/4)
}
breakpts <- seq(m.TotalCharges/4, m.TotalCharges, m.TotalCharges/4); breakpts
df$c.TotalCharges <- factor(df$c.TotalCharges, labels = c("(-1,2171]",
                                                          "(2171,4342]", 
                                                          "(4342,6514]",
                                                          "(6514,8685]"))
summary(df$c.TotalCharges)
par(mfrow=c(1,2))
plot(df$c.TotalCharges, main = "Barplot of df$c.TotalCharges")
hist(df$TotalCharges)
```

```{r Discretize MonthlyCharges}
df$c.MonthlyCharges <- df$MonthlyCharges
m.MonthlyCharges <- max(df$MonthlyCharges, na.rm = TRUE)
df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, df$MonthlyCharges <= m.MonthlyCharges/4, m.MonthlyCharges/4)
for (i in 1:3) {
  idx <- (m.MonthlyCharges*i/4 < df$MonthlyCharges) & (df$MonthlyCharges <=
                                                       m.MonthlyCharges*(i+1)/4)
  df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, idx,
                                 m.MonthlyCharges*(i+1)/4)
}
min(df$MonthlyCharges, na.rm = TRUE)
breakpts <- seq(m.MonthlyCharges/4, m.MonthlyCharges, m.MonthlyCharges/4)
breakpts
df$c.MonthlyCharges <- factor(df$c.MonthlyCharges, labels = c("(18,30.69]",
                                                          "(30.69,59.38]", 
                                                          "(59.38,89.06]",
                                                          "(89.06,118.75]"))
summary(df$c.MonthlyCharges)
par(mfrow=c(1,2))
plot(df$c.MonthlyCharges, main = "Barplot of df$c.MonthlyCharges")
hist(df$MonthlyCharges)
par(mfrow=c(1,1))
```

Lastly, we identified categorical and numerical variables for later use.

```{r Identify categorical and numerical variables}
numeric_val_idx = which(sapply(df, is.numeric))
numeric_val = names(df)[numeric_val_idx]
# The only numerical features that we have are tenure, MonthlyCharges and TotalChages.

# So the remaining will be categorical features.
categoric_val_idx = which(sapply(df, is.factor))
categoric_val = names(df)[categoric_val_idx]
```

\newpage
# 2. Exploratory Data Analysis (EDA)

EDA was done mainly automatically using the "DataExplorer" library. It plots, for each variable, the distribution of numeric variables, the proportion of individuals in each category and the amount of missing values, among other metadata.

The main conclusions of this section are:
1- Using the QQ plots and distribution plots we see that no numerical variable is normally distributed. This was also checked visually and with Kolmogorov-Smirnov tests, a more suitable approach than Shappiro-Wilk for large samples.

2- Our database is not balanced in some categories, like PhoneService (`r round(prop.table(table(df$PhoneService))["No"]*100,0)`% of "No") or SeniorCitizen(`r round(prop.table(table(df$SeniorCitizen))["No"]*100,0)`% of "No"). This is specially relevant for the target, "Churn", that has `r round(prop.table(table(df$Churn))["No"]*100,0)`% of cases of "No", so individuals that churned will be more difficult to predict.

3- Qualitative variables have a maximum of 4 levels, so all of them may be suitable for modeling without any aggregation.

5- Some categories, like "OnlineSecurity" or "OnlineBackup", are not applicable if the client does not have an internet connection. Consequently, there is a special level for those cases that contains around `r round(prop.table(table(df$InternetService))["No"]*100,0)`% of the clients. 

```{r EDA}
# Basic EDA
summary(df)

# Completed EDA 
#create_report(df, output_file = "Telco.html")
```

```{r Analysis of normality}
# tests
ks.test(df$TotalCharges, "pnorm")
ks.test(df$MonthlyCharges, "pnorm")
ks.test(df$tenure, "pnorm")

# plots
par(mfrow=c(1,2))
hist(df$tenure, prob = TRUE, breaks = 10, main = 'Histogram of tenure 
     vs normal distribution', xlab = 'tenure')
x <- seq(min(df$tenure), max(df$tenure), by = .1)
y <- dnorm(x, mean = mean(df$tenure), sd = sd(df$tenure))
plot(x,y, xlab = 'tenuere', ylab = '')

hist(df$TotalCharges, prob = TRUE, breaks = 10, main = 'Hist totalCharges 
     vs normal distribution', xlab = 'TotalCharges')
x <- seq(min(df$TotalCharges, na.rm = TRUE), max(df$TotalCharges, na.rm = TRUE),
         by = 10)
y <- dnorm(x, mean = mean(df$TotalCharges, na.rm = TRUE), sd = sd(df$TotalCharges, na.rm = TRUE))
plot(x,y, xlab = 'TotalCharges', ylab = '')

hist(df$MonthlyCharges, prob = TRUE, breaks = 10, main = 'Hist MonthlyCharges 
     vs normal distribution', xlab = 'df$MonthlyCharges')
x <- seq(min(df$MonthlyCharges, na.rm = TRUE), max(df$MonthlyCharges, na.rm = TRUE),
         by = .1)
y <- dnorm(x, mean = mean(df$MonthlyCharges, na.rm = TRUE), sd = sd(df$MonthlyCharges, na.rm = TRUE))
plot(x,y, xlab = 'df$MonthlyCharges', ylab = '')

par(mfrow=c(1,1))
```

\newpage
# 3. Data Quality Report

In this section we analysed the missing values, outliers and errors of numeric variables to increase the quality of data before modeling.

To start with, we detected that only "TotalCharges", and hence "c.TotalCharges", has a total of 22 missing observations. However, all of them correspond to new clients who have not receive their first invoice yet, so "TotalCharges" can not have a value. In other words, they are "not applicable cases". We naturally impute this observations with 0. 

```{r Distribution of missings}
# Distribution of missings in df per variable
apply(sapply(df, is.na), 2, sum)

# Distribution of missings in df per individual
table(apply(sapply(df, is.na), 1, sum))

# Check that all missings in "TotalCharges" correspond to individuals tenure = 0
TotalCharges.na <- which(is.na(df$TotalCharges))
sum(TotalCharges.na == which(df$tenure == 0)) == length(TotalCharges.na)

# So we transform them after creating a new numeric variable with all the missings of the database
df$n.na <- apply(sapply(df, is.na), 1, sum)

df$TotalCharges[TotalCharges.na] = 0
df$c.TotalCharges[TotalCharges.na] = "(-1,2171]"
```

Secondly, we detected data inconsistencies. For categorical values, we checked the EDA automatic reports and the summaries to ensure that all qualitative variables categories were meaningful and that there was not any misspelling errors. We also checked that all values of numeric variables were positive and reasonable.

Additionally, for "TotalCharges" we ensured that all the values were correct by manually calculating the value and comparing it to the actual total charge.

```{r Compare TotalCharges to its expected values}
# Expected total charges as the product of monthly charges and tenure
expected_total_charges = df$MonthlyCharges * df$tenure

# Plot them against the actual total charges
plot(expected_total_charges, df$TotalCharges)
# There are no outliers, so TotalCharges is consistent.
```

Thirdly, we analysed univariate outliers in numeric variables using Boxplots and the typical thresholds: 1.5 * IQR(interquartile range) for mild outliers and 3 * IQR for severe outliers. As there were not any we considered that all points were suitable for our models.

```{r Boxplots}
par(mfrow=c(1, length(numeric_val_idx)))
for (var in as.numeric(numeric_val_idx)) {
  Boxplot(df[,var], ylab = names(df)[var], main = "Boxplot") 
}
par(mfrow=c(1,1))
```

## 3.1 In depth analysis of missing values

Next, we will compute for every group of individuals the mean of missing values. Then we will rank the groups according to the computed mean.

```{r Mean of missings for each group}
# c.TotalCharges has missings, so it doesn't make sense to compute the mean
# of missings in its categories

interesting_cat_idx <- categoric_val_idx[-c(1,20)]
k = 0
for (i in interesting_cat_idx){
  k <- k + length(levels(df[,i]))
}
groups.na <- matrix(0, k, 2)
l = 1
for (idx in interesting_cat_idx) {
  categories.na <- tapply(df$n.na, df[,idx], mean)
  for (j in seq(length(categories.na))) {
    groups.na[l + j - 1,] <- c(categories.na[j],
                               paste(names(df)[idx], levels(df[,idx])[j],
                                     sep = "."))
  }
  l <- l + j
}
groups.na.df <- data.frame(na.perc = groups.na[,1], group = groups.na[,2])
groups.na.df[order(groups.na.df$na.perc, decreasing = TRUE),]
```

The groups with the highest proportion of missing data are made of those individuals who:

- Have a two-year contract
- Have dependents
- Pay with a mailed check

Since the set of individuals with missing data is exactly that of the new clients, we conclude that recently incorporated clients tend to: sign a two-year contract, have dependents and pay with a mailed check.

We can compute as well the pearson correlation coefficient between "n.na" and the numerical variables.

```{r n.na correlations}
# Creation of the correlation matrix
corr_mat <- cor(df[,c(numeric_val_idx, 25)],)
corr_mat

corrplot(corr_mat, order = 'hclust', tl.cex = 0.9)
```

n.na is independent to the rest of numerical variables, probably because it evaluates to 0 in most observations.

```{r Remove n.na, echo=F, message=FALSE, warning=FALSE, results='hide'}
# We remove n.na so that it does not interfere with the rest of the project
df$n.na <- NULL
```

## 3.2 Multivariate outliers

In this section we focused on detecting the multivariate outliers using "Moutlier". We discovered 344 multivariate outliers, about 5% of the individuals, as it was expected. We decided to maintain them and only remove them in the modeling step if they turned out to be influential points.

```{r Moutlier}
set.seed(123)
res.mout <- Moutlier(df[,numeric_val_idx], quantile = 0.95, plot= FALSE)

# Visual representation
par(mfrow=c(1,2), cex.main=0.8)
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable 
outliers', xlab= 'Observation', 
     ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)

plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation', 
     ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1), cex.main=1)

# Identification of the outliers
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff) 
length(outliers)
length(outliers)/dim(df)[1]*100
```

\newpage
# 4. Profiling of the target and feature selection

## Numeric variables' correlations
We analysed the pearson correlation coefficient to detect variables that were highly related and not include them all in the model. In the correlation plot of section 3.1 we see that "TotalCharges" is highly correlated with "MonthlyCharges" and "tenure" as the first one is calculated as the product of the others.

## Profiling of the target
Later on, we profiled the target Churn using a custom function "profiling()" created in the Multivariate Analysis subject of the Master's degree. This method expands "catdes()" and performs many plots and tests according to the type of each variable. We will focus on plots and the given tests' results: Chi^2, ANOVA and Kruskal-Wallis, which can be found in the annex.

```{r Karinas profiling method, results='hide', fig.show='hide'}
# Analysis of all variables except the ID
profiling(df[-c(grep("customerID", names(df)), grep("Churn", names(df)))], df$Churn, "Churn")
```

The most relevant conclusions are:
- Some variables are not significant, like Gender (Chi^2 p-value=0.4866) or Phone service (Chi^2 p-value=0.3388). Consequently, we state that churn is independent of the client's gender and whether he/she/they has a phone service contracted.

```{r targets profiling with gender and PhoneService, results='hide', fig.show='hide'}
profiling(df[c(grep("gender", names(df)),grep("PhoneService", names(df)))], df$Churn, "Churn")
```

- There are variables like "MultipleLines" that even being significant (Chi^2 p-value=0.003464) the difference among levels is small, as we can see in the plots

```{r targets profiling with MultipleLines, results='hide', fig.show='hide'}
profiling(df[grep("MultipleLines", names(df))], df$Churn, "Churn")
```

- The rest of variables, including the discretized, have a small p-value (< 2.2e-16) in the Chi^2, ANOVA or Kruskal-Wallis test, according to their type, and have at least one level where the target's distribution is different than in the rest. For example, 40% of people that did not have an online backup churned, while only 22% of customers having the backup did.

```{r targets profiling with OnlineBackup, results='hide', fig.show='hide'}
profiling(df[grep("OnlineBackup", names(df))], df$Churn, "Churn")
```

## Feature Selection
Finally, we decided which variables were suitable to be included in the model. 

The id was removed, since it will not give us any knowledge nor be useful to predict the target. 

```{r Remove customerID}
df$customerID <- NULL
```

We then computed the relationship between all the variables and the target with the "catdes()" method and chose the most relevant of them for the target's explanation.

All p-values of the Chi-squared test for categorical variables are very low, less than 0.001. The 6 variables with the lowest p-value are Contract, OnlineSecurity, TechSupport, c.tenure, InternetService, PaymentMethod. Note that the list includes a discretized numerical variable.

```{r catdes categorical}
# Correlation between all variables and our qualitative target Churn.
res.cat = catdes(df, grep("Churn", names(df)))

# Most important categorical variables, sorted by p value
res.cat$test.chi2
```

As for numeric variables, "tenure" has the smallest p-value in the F-test, much lower than those of discrete variables. As we have already seen, there is a high correlation between "MonthlyCharges", "tenure" and "TotalCharges" so we will only include in the models "TotalCharges" or "MonthlyCharges" together with "tenure".

```{r catdes numerical}
res.cat$quanti.var
```

## Profiling of the target with the selected categorical features

Lastly, we decided to make an extensive profiling of the six categorical variables that we could use in the model in order to understand them better. The main conclusions for each variable were:

- Contract: The probability of churning is decreased when the contract term increases. For example, if a costumer has a month contract and changes it to an annual the probability of not churning increases from 0.58 to 0.89.

- InternetService: People that do not have an internet service do not usually churn (7%). However, if they had a Fiber optic connection, the probability to churn increases (42%). This could be explained by the fact that users with a fast internet connection try to get the best offer for the service, but it would be necessary to make a market analysis to validate this hypothesis.

- OnlineSecurity: The probability of churning is small when the customer has online security. However, having an internet connection or not seems a more interesting feature than the variable itself, as the "No internet service" level has the smallest p-value.

- TechSupport: Having tech support increases the probability of not churning from 60% to 84% (when compared with not having it, although having internet service). Having internet service or not is, again, a more relevant feature.

- c.tenure: Loyalty is important, since people tend to churn less when they have spent longer with the service. For example, people who have spent less than 1.5 years has churned 44% of times, but only 8% of those who have stayed for more than 4.5 years have churned. 

- PaymentMethod: The proportion of people that churned is very similar in all types of payment except for "Electronic check". In this level, the proportion of churns is 45%, 18% higher than the global average.

```{r Extensive profiling of most relevant categorical variables}
# Global proportions of Churn categories
proportions(table(df$Churn))

# Calculate the indexes of the variables to investigate
names = c("Contract", "OnlineSecurity", "TechSupport", "c.tenure", "InternetService", "PaymentMethod")
index = NULL

for (i in 1:length(names)) {
  ind = grep(names[i], colnames(df))
  index = append(index, ind)
}
index = append(index, grep("Churn", names(df)))

# Profiling of only those variables
res.cat2 = catdes(df[,index], length(index))

res.cat2$category

# Another visualization of the profiling
#profiling(df[,index], df$Churn, "Churn")
```

\newpage
# 5. Modeling 

## Data splitting

First, let's split the dataset into training and testing set. We have decided that 70% of the data will be used for training.

```{r Data splitting}
set.seed(123)

sampling = sample.split(df$Churn, SplitRatio = 0.7)
train = subset(df, sampling == TRUE)
test = subset(df, sampling == FALSE)
```

## Modeling only with numerical variables

As we mentioned, there is a strong correlation between {tenure, MonthlyCharges} and TotalCharges, as the second one is simply the product of the variables in the first set. Hence,  we will build two models, one for each set of variables, and keep the best one.

```{r Models with numeric variables}
m0.set1 = glm (Churn ~ tenure + MonthlyCharges, data = train, family = binomial)
# Checking the Anova test, both variables are significant to our model.
# Hence, we won't remove any of them.
Anova(m0.set1, test = "LR")

m0.set2 = glm (Churn ~ TotalCharges, data = train, family = binomial)

BIC(m0.set1, m0.set2)
```

Checking the Bayesian criterion, the set {tenure, MonthlyCharges} has a much lower value and its variables are significant. Hence, we'll choose this set of variables for further analysis.


We also check possible transformation for our model m0.set1.

```{r Transformations of numeric variables in the model}
m0.log = glm (Churn ~ tenure + log(MonthlyCharges), data = train, family = binomial)
m0.sqrt = glm (Churn ~ sqrt(tenure) + MonthlyCharges, data = train, family = binomial)

BIC (m0.set1, m0.log, m0.sqrt)
```

We have tried several transformations for both variables (sqrt, log, exp, etc), but BIC shows that the best model is the one with sqrt on tenure.

Discretized variables might create a better model, so we study this possibility.

```{r Model with discretized numerical variables}
m1 = glm (Churn ~ c.tenure + MonthlyCharges, data = train, family = binomial)

BIC(m1, m0.sqrt)
```

Checking the AIC and BIC parameters, we decided to keep the numerical version of tenure. We have checked as well the model with MonthlyCharges discretized, but the AIC is worse once more.

## Residual analysis only with numerical variables

It is important to look for influential points in the model that could worsen it. "influencePlot()" computes the Cook's distance of each point, so that we can compare them with the threshold studied in the course.

```{r Influential data on model with numerical variables, fig.show='hide'}
# Check influential points
influent = influencePlot(m0.sqrt)[3]; influent

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
```

The Cook's distances obtained from "influencePlot()" are smaller than our threshold, so we will not remove any point.

## Adding factor main effects to the model

After being satisfied with our final model based on numerical variables, we add categorical variables to it in decreasing relevance order.

```{r Adding {Contract}}
# Adding {Contract}
m2 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract, 
          data = train, family = binomial)

# Adding {contract} indeed reduces the BIC of our model.
BIC(m0.sqrt, m2)
```

```{r Adding {InternetService}}
# Adding {InternetService}
m3 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService, 
          data = train, family = binomial)

# Adding {InternetService} indeed reduces the BIC of our model.
BIC(m2,m3)
```

We have figured out in the profiling section that {InternetService} and {OnlineSecurity, TechSupport} have some levels that are strongly correlated.
Specifically, when "InternetService" = "No", "OnlineSecurity and "TechSupport" 
can't be given a value, so they are declared as "No intervet service".

To avoid multicollinearity and NA's, we need to decide which variable to keep.

```{r Adding {TechSupport, OnlineSecurity}}
# Adding {TechSupport, OnlineSecurity}
m4 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + OnlineSecurity 
          + TechSupport, data = train, family = binomial)

BIC(m3, m4)
```

The BIC criterion for m4 is smaller, but taking into account that "InternetService" is more correlated with the target variable and the difference in the BIC is not that significant, we decided to keep m3, with "InternetService".

```{r Adding {PaymentMethod}}
# Adding {PaymentMethod}
m5 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod, data = train, family = binomial)

# Adding {PaymentMethod} indeed reduces the BIC of our model.
BIC(m3, m5)
```

## Residual analysis with categorical variables

We repeat the residual analysis performed earlier with our current model.

```{r Influential data on model with categorical variables, fig.show='hide'}
# Check influential points
influent = influencePlot(m5)[3]; influent

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
```

As before, the Cook's distances obtained from "influencePlot()" are smaller than our threshold, so we will not remove any point.

## Adding interactions to the model

Sometimes interactions between dependent variables improve a model, so let us see how they work in our case. To start with, we check all possible interactions and execute "step()" to end up with the most relevant ones.

```{r Check all possible interactions}
# Check all possible interactions of model m5
m6 = glm (Churn ~ (sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod)^2, data = train, family = binomial)

# Use step function to find the combination that minimizes the AIC. 
step(m6)
```

Even though "step()" recommends not to add any interaction, we see how the ones with the smallest AIC perform. That is, we add the interactions between "sqrt(tenure)" and "PaymentMethod" or "Contract".

```{r sqrt(tenure):PaymentMethod and sqrt(tenure):Contract}
m7 = glm(Churn ~ sqrt(tenure) * PaymentMethod + sqrt(tenure) * Contract +
          MonthlyCharges + InternetService + PaymentMethod, data = train, 
          family = binomial)

BIC (m5, m7)
```

According to the BIC criterion, no improvement is obtained.

Now we will add the interaction with the highest AIC instead, 
"MonthlyCharges:InternetService".

```{r MonthlyCharges:InternetService}
m8 = glm(Churn ~ sqrt(tenure) +  Contract + MonthlyCharges * InternetService 
         + PaymentMethod, data = train, family = binomial)

BIC(m5,m8)
summary(m8)
```

The BIC improved from 4174 to 4164, but with the cost of 2 degrees of freedom. Adding the interaction between "MonthlyCharges" and "InternetService" is a trade-off between simplicity and accuracy. At this point, after having added many variables, we value more simplicity, so we will not add this interaction.

## Trying link function probit

We are interested in the effect of changing the link function of the logistic regression to probit.

```{r probit}
m9 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod, data = train, family = binomial(link = "probit"))

BIC(m5, m9)
```

Sadly, based on the BIC criterion, no improvement is obtained.

## Final residual analysis

We will perform now a final residual analysis.

```{r Final Influential data analysis}
# Check influential points
influent = influencePlot(m9)[3]; influent

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
```

Observations 269 and 4273 may be influential points, but both of them are smaller than the threshold.

\newpage
# 6. Goodness of fit

NOT FINISHED FROM HERE UNTIL THE ANNEX

```{r Residual plots}
residualPlots(m5)
```

## Model prediction

```{r predict()}
# First, we compute the probability of Churn for each observation (from test) 
# with predict function.
predictions = predict(m5, test, type = "response")

# Then, for those that have a probability higher than 0.5, we can consider
# Churn == "Yes"
probability = ifelse(predictions >= 0.5, "Yes", "No")

# Finally, compute the Confusion Matrix of predicted result.
CM = table(test$Churn, probability, dnn = c("Actual Churn", "Predicted Churn")); CM

accuracy = sum(diag(CM))/dim(test)[1]*100; accuracy

roc.curve(test$Churn, probability)
```


```{r PseudoR2()}
library(DescTools)
PseudoR2(m5, which = "McFadden")
```

\newpage
# 7. Model interpretation

\newpage
# Annex

## Expanded profiling of the target with the "profiling()" method

```{r Karinas profiling method not hidden}
# Analysis of all variables except the ID
profiling(df[-c(grep("customerID", names(df)), grep("Churn", names(df)))], df$Churn, "Churn")
```

