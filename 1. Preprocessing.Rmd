---
title: "SIM Project 2. Preprocessing"
author: "Adrià Casanova, Víctor Garcia, Zhengyong Ji"
date: "2024-01-05"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: no
editor_options:
  chunk_output_type: console
---

- Residual analysis at the end of the project.
- Profiling: AN EXTRA POINT TO BE DONE. WE CAN DO IT LATTER. USE THE SCRIPT PROVIDED BY MVA. 

```{r, echo=F, message=FALSE, warning=FALSE, results='hide'}
# Clean the workspace
if(!is.null(dev.list())) dev.off()
rm(list = ls())

library(dplyr) 
library(car)
library(DataExplorer)
library(FactoMineR)
library(caTools)
library(chemometrics)
library(corrplot)
source("profiling_func.R")
```

```{r}
# Import the dataset. Previopusly we have imported the required libraries
df = read.csv("WA_Fn-UseC_-Telco-Customer-Churn.xls",header=T, sep=",",
              stringsAsFactors=TRUE)
str(df)
summary(df)
```

# 1. Data preparation

The first part of the project consisted on doing some basic data preparation to ensure that data was ready for the next sections.

Firstly, we cheeked that all datatypes were consistent with the metadata and declared "SeniorCitizen" as a factor, as it represented a qualitative concept.

```{r}
# Transformation of "SeniorCitizen" to categorical
df$SeniorCitizen <- factor(df$SeniorCitizen, labels = c("Yes", "No"))
```

Secondly, we created discrete variables for all the numeric variables by splitting data into 4 categories. The limits of them were obtained simply by dividing the range in 4 equal intervals and the distribution was checked using histograms to ensure that it was similar to the original variable.

```{r}
# Discretize numerical variables

# tenure
df$c.tenure <- df$tenure # Create a new variable called Categorical.tenure
m.tenure <- max(df$tenure, na.rm = TRUE)
df$c.tenure <- replace(df$c.tenure, df$tenure <= m.tenure/4, m.tenure/4)
for (i in 1:3) {
  idx <- (m.tenure*i/4 < df$tenure) & (df$tenure <= m.tenure*(i+1)/4)
  df$c.tenure <- replace(df$c.tenure, idx, m.tenure*(i+1)/4)
}
min(df$tenure, na.rm = TRUE)
breakpts <- seq(m.tenure/4, m.tenure, m.tenure/4); breakpts
df$c.tenure <- factor(df$c.tenure, labels = c("(-1,18]", "(18,36]",
                                              "(36,54]", "(54,72]"))
summary(df$c.tenure)
par(mfrow=c(1,2))
plot(df$c.tenure, main = "Barplot of df$c.tenure")
hist(df$tenure)
```

```{r}
# TotalCharges
df$c.TotalCharges <- df$TotalCharges
m.TotalCharges <- max(df$TotalCharges, na.rm = TRUE)
df$c.TotalCharges <- replace(df$c.TotalCharges, df$TotalCharges <= m.TotalCharges/4, m.TotalCharges/4)
for (i in 1:3) {
  idx <- (m.TotalCharges*i/4 < df$TotalCharges) & (df$TotalCharges <=
                                                     m.TotalCharges*(i+1)/4)
  df$c.TotalCharges <- replace(df$c.TotalCharges, idx, m.TotalCharges*(i+1)/4)
}
breakpts <- seq(m.TotalCharges/4, m.TotalCharges, m.TotalCharges/4); breakpts
df$c.TotalCharges <- factor(df$c.TotalCharges, labels = c("(-1,2171]",
                                                          "(2171,4342]", 
                                                          "(4342,6514]",
                                                          "(6514,8685]"))
summary(df$c.TotalCharges)
par(mfrow=c(1,2))
plot(df$c.TotalCharges, main = "Barplot of df$c.TotalCharges")
hist(df$TotalCharges)
```

```{r}
# MonthlyCharges
df$c.MonthlyCharges <- df$MonthlyCharges
m.MonthlyCharges <- max(df$MonthlyCharges, na.rm = TRUE)
df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, df$MonthlyCharges <= m.MonthlyCharges/4, m.MonthlyCharges/4)
for (i in 1:3) {
  idx <- (m.MonthlyCharges*i/4 < df$MonthlyCharges) & (df$MonthlyCharges <=
                                                       m.MonthlyCharges*(i+1)/4)
  df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, idx,
                                 m.MonthlyCharges*(i+1)/4)
}
min(df$MonthlyCharges, na.rm = TRUE)
breakpts <- seq(m.MonthlyCharges/4, m.MonthlyCharges, m.MonthlyCharges/4)
breakpts
df$c.MonthlyCharges <- factor(df$c.MonthlyCharges, labels = c("(18,30.69]",
                                                          "(30.69,59.38]", 
                                                          "(59.38,89.06]",
                                                          "(89.06,118.75]"))
summary(df$c.MonthlyCharges)
par(mfrow=c(1,2))
plot(df$c.MonthlyCharges, main = "Barplot of df$c.MonthlyCharges")
hist(df$MonthlyCharges)
par(mfrow=c(1,1))
```

Lastly, we divided variables into categorical and numerical to use them in 
other sections.

```{r}
# The only numerical features that we have are tenure, MonthlyCharges and TotalChages.
numeric_val_idx = which(sapply(df, is.numeric))
numeric_val = names(df)[numeric_val_idx]

# So the remaining will be categorical features.
categoric_val_idx = which(sapply(df, is.factor))
categoric_val = names(df)[categoric_val_idx]
```


# 2. Exploratory Data Analysis (EDA)

EDA was done mainly automatically using "DataExplorer" library, which does, for each variable, basic plots that allows us to know the distribution of numeric variables, % for each category or missing counts, among other things.

The main conclusions of this section are:
1- Using the QQ plots and distribution plots we see that all numeric variables do not have a normal distribution. This was also checked using visual representation and Kolmogorov-Smirnov test, a more suitable approach than Shappiro Wilk for large samples.

2- Our database is not balanced in some categories, like PhoneService (9% of "No") or SeniorCitizen(16% of "No"). This is specially relevant for the target, "Churn", that has 30% of cases of "No", so individuals that churned will be more difficult to predict.

3- Qualitative variables have a maximum of 4 levels, so all of them may be suitable for modeling without any aggregation of levels.

4- Imputation is not required as there are mainly qualitative variables without missings. Missings will also be discussed in the next section.

```{r}
# Basic EDA
summary(df)

# Completed EDA 
#create_report(df, output_file = "Telco.html")
```

```{r}
# Analysis of normality
# tests:
ks.test(df$TotalCharges, "pnorm")
ks.test(df$MonthlyCharges, "pnorm")
ks.test(df$tenure, "pnorm")

# visually
par(mfrow=c(1,2))
hist(df$tenure, prob = TRUE, breaks = 10, main = 'Histogram of tenure 
     vs normal distribution', xlab = 'tenure')
x <- seq(min(df$tenure), max(df$tenure), by = .1)
y <- dnorm(x, mean = mean(df$tenure), sd = sd(df$tenure))
plot(x,y, xlab = 'tenuere', ylab = '')

hist(df$TotalCharges, prob = TRUE, breaks = 10, main = 'Hist totalCharges 
     vs normal distribution', xlab = 'TotalCharges')
x <- seq(min(df$TotalCharges, na.rm = TRUE), max(df$TotalCharges, na.rm = TRUE),
         by = 10)
y <- dnorm(x, mean = mean(df$TotalCharges, na.rm = TRUE), sd = sd(df$TotalCharges, na.rm = TRUE))
plot(x,y, xlab = 'TotalCharges', ylab = '')

hist(df$MonthlyCharges, prob = TRUE, breaks = 10, main = 'Hist MonthlyCharges 
     vs normal distribution', xlab = 'df$MonthlyCharges')
x <- seq(min(df$MonthlyCharges, na.rm = TRUE), max(df$MonthlyCharges, na.rm = TRUE),
         by = .1)
y <- dnorm(x, mean = mean(df$MonthlyCharges, na.rm = TRUE), sd = sd(df$MonthlyCharges, na.rm = TRUE))
plot(x,y, xlab = 'df$MonthlyCharges', ylab = '')

par(mfrow=c(1,1))
```

# 2. Data Quality Report

In this section we analysed the missing values, outliers and errors on the numeric variables to increase the quality of the data before modelling.

Firstly, regarding missings, we detected that only "TotalCharges" and, hence, "c.TotalCharges", have a total of 22 missing observations. However, all of them are "not applicable cases" as corresponds to new clients that, consequently, have not receive their first invoice yet so "TotalCharges" can not have a value.In this situation, instead of NA, we can convert them to 0. 

```{r}
# Distribution of missings in df per variable
apply(sapply(df, is.na), 2, sum)

# Distribution of missings in df per individual
table(apply(sapply(df, is.na), 1, sum))

# Here we check that all missings in "TotalCharges" are individuals with 0 in "tenure"´
TotalCharges.na <- which(is.na(df$TotalCharges))
sum(TotalCharges.na == which(df$tenure == 0)) == length(TotalCharges.na)

# So we transform them after creating a new numeric variable with all the missings of the database
df$n.na <- apply(sapply(df, is.na), 1, sum)

na.idx <- which(is.na(df$TotalCharges))
df$TotalCharges[na.idx] = 0
df$c.TotalCharges[na.idx] = "(-1,2171]"
```

Secondly, we detected data inconsistencies. For categorical values, we checked the EDA automatic reports and the summaries to ensure that all qualitative variables categories were meaningful and that there was not any misspelling errors. Additionally, we checked that all binary variables only contained 0 or 1 and that all values of numeric variables were positive and reasonable.

Additionally, for "TotalCharges" we ensured that all the values were correct by manually calculating the value, for each individual (monthly_charges * tenure) and comparing the value to the actual total charges.

```{r}
# Expected total charges as the product of monthly charges and tenure
expected_total_charges = df$MonthlyCharges * df$tenure

# Then plot them against the actual total charges
plot(expected_total_charges, df$TotalCharges)
# There are no outliers, so TotalCharges is consistent.
```

Thirdly, we analysed univariate outliers in numeric variables using Boxplots and the typical thresholds: 1.5 * IQR(interquartile range) for severe outliers and 3*IQR for severe outliers. As there were not any outliers we considered that all points were correct. 

```{r}
for (var in as.numeric(numeric_val_idx)) {
  Boxplot(df[,var], ylab = names(df)[var], main = "Boxplot") 
}
```

# 3. Creation of new variables??

Next, we will compute for every group of individuals the mean of missing values. Then we will rank the groups according to the computed mean.

```{r}
# c.TotalCharges has missings, so it doesn't make sense to compute the mean
# of missings in its categories

interesting_cat_idx <- categoric_val_idx[-c(1,20)]
k = 0
for (i in interesting_cat_idx){
  k <- k + length(levels(df[,i]))
}
groups.na <- matrix(0, k, 2)
l = 1
for (i in seq(length(interesting_cat_idx))) {
  idx <- interesting_cat_idx[i]
  categories.na <- tapply(df$n.na, df[,idx], mean)
  for (j in seq(length(categories.na))) {
    groups.na[l + j - 1,] <- c(as.numeric(categories.na[j]),
                               paste(names(df)[idx], levels(df[,idx])[j],
                                     sep = "."))
  }
  l <- l + j
}
groups.na.df <- data.frame(na.perc = groups.na[,1], group = groups.na[,2])
groups.na.df[order(groups.na.df$na.perc, decreasing = TRUE),]
```

# 4. Multivariate outliers

In this section we focused on detecting the multivariate outliers using "Moutlier" method. We discovered that there were 344 multivariate outliers, about 5% of the individuals as expect. However, we decided to maintain them and only remove them in the modeling step if they are influential points.

```{r}
set.seed(123)
res.mout <- Moutlier(df[,numeric_val_idx], quantile = 0.95, plot= FALSE)

# Visual representation
par(mfrow=c(1,2))
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable 
outliers', xlab= 'Observation', 
     ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)

plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation', 
     ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))

# Identification of the outliers
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff) 
length(outliers)
length(outliers)/dim(df)[1]*100
```

# Profiling and feature selection

## Numeric variables
Firstly, we analysed the pearson correlation coefficient to detect variables that were highly related and not include them in the model. We have also included two variables "n.na", that contained the total number missing values per individual. Note that, as there are not any outliers or errors, it makes no sense to define those variables as it would be a vector of 0's with, obviously, no correlation with other variables.

In the correlation plot we see that "TotalCharges" is highly correlated with "MonthlyCharges" and "tenure" as the first one is calculated as the product of the others. n.na is not correlated to any other variable as it only contains 11 individuals that are not 0.

```{r}
# Creation of the correlation matrix
corr_mat <- cor(df[,c(numeric_val_idx, 25)],)
corr_mat

corrplot(corr_mat, order = 'hclust')

# We remove the newly created variables to  not interfere in the rest of the project
df$n.na <- NULL
```

## Categorical variables

Secondly, we 


We performed a profiling using "gtable" which allows us to check the proportions of
each category visually and numerically, as it performs a "Pearson's Chi-squared test"
for each variable.

For each variable, the most relevant conclusions are:
-gender:

```{r}
profiling(df[-1], df$Churn, "Churn")
```


## Feature Selection
Lastly, we decided which variables were suitable to be included in the model. 

Firstly, we removed the "id" as we consider that it will not give us any useful knowledge or be useful to predict the target. Secondly, we computed the relationship of variables against the target.

```{r}
# customerID should be removed
df$customerID <- NULL

# Execute catdes function to find the correlation between the variables 
# against our qualitative target Churn.
res.cat = catdes(df, 20)

# Through sqrt_chi test, we can rank the most influential feature.
res.cat$test.chi2

# The outcome of Chi sqrt test proved that all the categorical feature are meanfull (p-value > 0.05)
# The key feature are the "Contract" period type, followed by "OnlineSecurity" and the "TechSupport".

res.cat$quanti.var
# And the correlation of numerical variable are also important.
```

# 3. Modeling 


# 3.1. Using only using numeric variables

```{r}
# First, let's split the dataset into training and testing. 
# We can consider that 70% of data will be used for training purpose.

set.seed(123)

sampling = sample.split(df$Churn, SplitRatio = 0.7)
train = subset(df, sampling == TRUE)
test = subset(df, sampling == FALSE)
```


```{r}
# Modelling with only numerical variables.

m0.numerical = glm (Churn ~ tenure + TotalCharges, data = train, family = binomial)

summary(m0.numerical)

# Checking the Anova test, there is no variable excluded, which means that all of then are relevant.
Anova (m0.numerical)
```

```{r}
# Check the previous model, but with different transformations like logaritmit.

train_positive = replace(train, train == 0, 1e-6)
train_positive$Churn == as.numeric(train$Churn)


boxTidwell(Churn ~ tenure + TotalCharges, data = train_positive)
```



```{r}
m0.discrete = glm (Churn ~ c.tenure + c.TotalCharges, data = train, family = binomial)

summary (m0.discrete)

Anova (m0.numerical)
```


```{r}
# The first model will be with all the ORIGINAL variables.

m0 = glm (Churn ~., data = train[,1:20], family = binomial)
summary(m0)

# The second model will be with all the categorical variable, including the discretized value (the numerical's ones).

m0.c = glm (Churn ~., data = train[,-c(5,18,19)], family = binomial)
summary(m0.c)

# Compare the difference of these two model with AIC criteria.
AIC (m0, m0.c)

# The AIC parameter for both model are quite similar, although the m0 is slightly better. Hence, we'll use the original dataset, without discretizing the numerical variable.

# As the two model are not nested, we cannot apply the anova test
# anova (m0, m0.c, test = "Chisq")
```

The AIC criteria shows that using numerical variables or discretized variables cannot make much difference. In this situation, in order to reduce the workload, we decided to use the discretized variables (hence, we'll have only categorical variable)

```{r}
# Next step is to do the net-effect.
```



