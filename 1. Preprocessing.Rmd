---
title: "SIM Project 2. Preprocessing"
author: "Adrià Casanova, Víctor Garcia, Zhengyong Ji"
date: "2024-01-05"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: 3
    number_sections: no
editor_options:
  chunk_output_type: console
---

- Residual analysis at the end of the project.
- Profiling: AN EXTRA POINT TO BE DONE. WE CAN DO IT LATTER. USE THE SCRIPT PROVIDED BY MVA. 

```{r, echo=F, message=FALSE, warning=FALSE, results='hide'}
# Clean the workspace
if(!is.null(dev.list())) dev.off()
rm(list = ls())

library(dplyr) 
library(car)
library(DataExplorer)
library(FactoMineR)
library(caTools)
library(chemometrics)
library(corrplot)
source("profiling_func.R")
```

```{r}
# Import the dataset. Previopusly we have imported the required libraries
df = read.csv("WA_Fn-UseC_-Telco-Customer-Churn.xls",header=T, sep=",",
              stringsAsFactors=TRUE)
str(df)
summary(df)
```

# 1. Data preparation

The first part of the project consisted on doing some basic data preparation to ensure that data are ready for the next sections.

Firstly, we cheeked that all datatypes were consistent with the metadata and declared "SeniorCitizen" as a factor, as it represented a qualitative concept.

```{r}
# Transformation of "SeniorCitizen" to categorical
df$SeniorCitizen <- factor(df$SeniorCitizen, labels = c("Yes", "No"))
```

Secondly, we created discrete variables for all the numeric variables by splitting data into 4 categories. The limits of them were obtained simply by dividing the range in 4 equal intervals and the distribution was checked using histograms to ensure that it was similar to the original variable.

```{r}
# Discretize numerical variables

# tenure
df$c.tenure <- df$tenure # Create a new variable called Categorical.tenure
m.tenure <- max(df$tenure, na.rm = TRUE)
df$c.tenure <- replace(df$c.tenure, df$tenure <= m.tenure/4, m.tenure/4)
for (i in 1:3) {
  idx <- (m.tenure*i/4 < df$tenure) & (df$tenure <= m.tenure*(i+1)/4)
  df$c.tenure <- replace(df$c.tenure, idx, m.tenure*(i+1)/4)
}
min(df$tenure, na.rm = TRUE)
breakpts <- seq(m.tenure/4, m.tenure, m.tenure/4); breakpts
df$c.tenure <- factor(df$c.tenure, labels = c("(-1,18]", "(18,36]",
                                              "(36,54]", "(54,72]"))
summary(df$c.tenure)
par(mfrow=c(1,2))
plot(df$c.tenure, main = "Barplot of df$c.tenure")
hist(df$tenure)
```

```{r}
# TotalCharges
df$c.TotalCharges <- df$TotalCharges
m.TotalCharges <- max(df$TotalCharges, na.rm = TRUE)
df$c.TotalCharges <- replace(df$c.TotalCharges, df$TotalCharges <= m.TotalCharges/4, m.TotalCharges/4)
for (i in 1:3) {
  idx <- (m.TotalCharges*i/4 < df$TotalCharges) & (df$TotalCharges <=
                                                     m.TotalCharges*(i+1)/4)
  df$c.TotalCharges <- replace(df$c.TotalCharges, idx, m.TotalCharges*(i+1)/4)
}
breakpts <- seq(m.TotalCharges/4, m.TotalCharges, m.TotalCharges/4); breakpts
df$c.TotalCharges <- factor(df$c.TotalCharges, labels = c("(-1,2171]",
                                                          "(2171,4342]", 
                                                          "(4342,6514]",
                                                          "(6514,8685]"))
summary(df$c.TotalCharges)
par(mfrow=c(1,2))
plot(df$c.TotalCharges, main = "Barplot of df$c.TotalCharges")
hist(df$TotalCharges)
```

```{r}
# MonthlyCharges
df$c.MonthlyCharges <- df$MonthlyCharges
m.MonthlyCharges <- max(df$MonthlyCharges, na.rm = TRUE)
df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, df$MonthlyCharges <= m.MonthlyCharges/4, m.MonthlyCharges/4)
for (i in 1:3) {
  idx <- (m.MonthlyCharges*i/4 < df$MonthlyCharges) & (df$MonthlyCharges <=
                                                       m.MonthlyCharges*(i+1)/4)
  df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, idx,
                                 m.MonthlyCharges*(i+1)/4)
}
min(df$MonthlyCharges, na.rm = TRUE)
breakpts <- seq(m.MonthlyCharges/4, m.MonthlyCharges, m.MonthlyCharges/4)
breakpts
df$c.MonthlyCharges <- factor(df$c.MonthlyCharges, labels = c("(18,30.69]",
                                                          "(30.69,59.38]", 
                                                          "(59.38,89.06]",
                                                          "(89.06,118.75]"))
summary(df$c.MonthlyCharges)
par(mfrow=c(1,2))
plot(df$c.MonthlyCharges, main = "Barplot of df$c.MonthlyCharges")
hist(df$MonthlyCharges)
par(mfrow=c(1,1))
```

Lastly, we divided variables into categorical and numerical to use them in 
other sections.

```{r}
# The only numerical features that we have are tenure, MonthlyCharges and TotalChages.
numeric_val_idx = which(sapply(df, is.numeric))
numeric_val = names(df)[numeric_val_idx]

# So the remaining will be categorical features.
categoric_val_idx = which(sapply(df, is.factor))
categoric_val = names(df)[categoric_val_idx]
```


# 2. Exploratory Data Analysis (EDA)

EDA was done mainly automatically using "DataExplorer" library, which does, for each variable, basic plots that allows us to know the distribution of numeric variables, % for each category or missing counts, among other things.

The main conclusions of this section are:
1- Using the QQ plots and distribution plots we see that all numeric variables do not have a normal distribution. This was also checked using visual representation and Kolmogorov-Smirnov test, a more suitable approach than Shappiro Wilk for large samples.

2- Our database is not balanced in some categories, like PhoneService (9% of "No") or SeniorCitizen(16% of "No"). This is specially relevant for the target, "Churn", that has 30% of cases of "No", so individuals that churned will be more difficult to predict.

3- Qualitative variables have a maximum of 4 levels, so all of them may be suitable for modeling without any aggregation of levels.

4- Imputation is not required as there are mainly qualitative variables without missings. Missings will also be discussed in the next section.

5- Some categories, like "OnlineSecurity" or "OnlineBackup", are not applicable if the client does not have an internet connection. Consequently, there is an special level for those cases that represents around 30% of the categories. 

```{r}
# Basic EDA
summary(df)

# Completed EDA 
#create_report(df, output_file = "Telco.html")
```

```{r}
# Analysis of normality
# tests:
ks.test(df$TotalCharges, "pnorm")
ks.test(df$MonthlyCharges, "pnorm")
ks.test(df$tenure, "pnorm")

# visually
par(mfrow=c(1,2))
hist(df$tenure, prob = TRUE, breaks = 10, main = 'Histogram of tenure 
     vs normal distribution', xlab = 'tenure')
x <- seq(min(df$tenure), max(df$tenure), by = .1)
y <- dnorm(x, mean = mean(df$tenure), sd = sd(df$tenure))
plot(x,y, xlab = 'tenuere', ylab = '')

hist(df$TotalCharges, prob = TRUE, breaks = 10, main = 'Hist totalCharges 
     vs normal distribution', xlab = 'TotalCharges')
x <- seq(min(df$TotalCharges, na.rm = TRUE), max(df$TotalCharges, na.rm = TRUE),
         by = 10)
y <- dnorm(x, mean = mean(df$TotalCharges, na.rm = TRUE), sd = sd(df$TotalCharges, na.rm = TRUE))
plot(x,y, xlab = 'TotalCharges', ylab = '')

hist(df$MonthlyCharges, prob = TRUE, breaks = 10, main = 'Hist MonthlyCharges 
     vs normal distribution', xlab = 'df$MonthlyCharges')
x <- seq(min(df$MonthlyCharges, na.rm = TRUE), max(df$MonthlyCharges, na.rm = TRUE),
         by = .1)
y <- dnorm(x, mean = mean(df$MonthlyCharges, na.rm = TRUE), sd = sd(df$MonthlyCharges, na.rm = TRUE))
plot(x,y, xlab = 'df$MonthlyCharges', ylab = '')

par(mfrow=c(1,1))
```

# 2. Data Quality Report

In this section we analysed the missing values, outliers and errors on the numeric variables to increase the quality of the data before modelling.

Firstly, regarding missings, we detected that only "TotalCharges" and, hence, "c.TotalCharges", have a total of 22 missing observations. However, all of them are "not applicable cases" as corresponds to new clients that, consequently, have not receive their first invoice yet so "TotalCharges" can not have a value.In this situation, instead of NA, we can convert them to 0. 

```{r}
# Distribution of missings in df per variable
apply(sapply(df, is.na), 2, sum)

# Distribution of missings in df per individual
table(apply(sapply(df, is.na), 1, sum))

# Here we check that all missings in "TotalCharges" are individuals with 0 in "tenure"´
TotalCharges.na <- which(is.na(df$TotalCharges))
sum(TotalCharges.na == which(df$tenure == 0)) == length(TotalCharges.na)

# So we transform them after creating a new numeric variable with all the missings of the database
df$n.na <- apply(sapply(df, is.na), 1, sum)

na.idx <- which(is.na(df$TotalCharges))
df$TotalCharges[na.idx] = 0
df$c.TotalCharges[na.idx] = "(-1,2171]"
```

Secondly, we detected data inconsistencies. For categorical values, we checked the EDA automatic reports and the summaries to ensure that all qualitative variables categories were meaningful and that there was not any misspelling errors. Additionally, we checked that all binary variables only contained 0 or 1 and that all values of numeric variables were positive and reasonable.

Additionally, for "TotalCharges" we ensured that all the values were correct by manually calculating the value, for each individual (monthly_charges * tenure) and comparing the value to the actual total charges.

```{r}
# Expected total charges as the product of monthly charges and tenure
expected_total_charges = df$MonthlyCharges * df$tenure

# Then plot them against the actual total charges
plot(expected_total_charges, df$TotalCharges)
# There are no outliers, so TotalCharges is consistent.
```

Thirdly, we analysed univariate outliers in numeric variables using Boxplots and the typical thresholds: 1.5 * IQR(interquartile range) for severe outliers and 3*IQR for severe outliers. As there were not any outliers we considered that all points were correct. 

```{r}
for (var in as.numeric(numeric_val_idx)) {
  Boxplot(df[,var], ylab = names(df)[var], main = "Boxplot") 
}
```

# 3. Creation of new variables??

Next, we will compute for every group of individuals the mean of missing values. Then we will rank the groups according to the computed mean.

```{r}
# c.TotalCharges has missings, so it doesn't make sense to compute the mean
# of missings in its categories

interesting_cat_idx <- categoric_val_idx[-c(1,20)]
k = 0
for (i in interesting_cat_idx){
  k <- k + length(levels(df[,i]))
}
groups.na <- matrix(0, k, 2)
l = 1
for (i in seq(length(interesting_cat_idx))) {
  idx <- interesting_cat_idx[i]
  categories.na <- tapply(df$n.na, df[,idx], mean)
  for (j in seq(length(categories.na))) {
    groups.na[l + j - 1,] <- c(as.numeric(categories.na[j]),
                               paste(names(df)[idx], levels(df[,idx])[j],
                                     sep = "."))
  }
  l <- l + j
}
groups.na.df <- data.frame(na.perc = groups.na[,1], group = groups.na[,2])
groups.na.df[order(groups.na.df$na.perc, decreasing = TRUE),]
```

# 4. Multivariate outliers

In this section we focused on detecting the multivariate outliers using "Moutlier" method. We discovered that there were 344 multivariate outliers, about 5% of the individuals as expect. However, we decided to maintain them and only remove them in the modeling step if they are influential points.

```{r}
set.seed(123)
res.mout <- Moutlier(df[,numeric_val_idx], quantile = 0.95, plot= FALSE)

# Visual representation
par(mfrow=c(1,2))
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable 
outliers', xlab= 'Observation', 
     ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)

plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation', 
     ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))

# Identification of the outliers
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff) 
length(outliers)
length(outliers)/dim(df)[1]*100
```

# 5. Profiling and feature selection

## Numeric variables
Firstly, we analysed the pearson correlation coefficient to detect variables that were highly related and not include them in the model. We have also included two variables "n.na", that contained the total number missing values per individual. Note that, as there are not any outliers or errors, it makes no sense to define those variables as it would be a vector of 0's with, obviously, no correlation with other variables.

In the correlation plot we see that "TotalCharges" is highly correlated with "MonthlyCharges" and "tenure" as the first one is calculated as the product of the others. n.na is not correlated to any other variable as it only contains 11 individuals that are not 0.

```{r}
# Creation of the correlation matrix
corr_mat <- cor(df[,c(numeric_val_idx, 25)],)
corr_mat

corrplot(corr_mat, order = 'hclust')

# We remove the newly created variables to  not interfere in the rest of the project
df$n.na <- NULL
```

## Categorical variables

Secondly, we performed a profiling using "gtable" which allows us to check if there are significant differences of the levels of a variable and the target.This is done visually and using  a "Pearson's Chi-squared test". 

```{r, results='hide'}
# Analysis of all variables except the ID
profiling(df[-1], df$Churn, "Churn")
```

For each variable, the most relevant conclusions are:
- Some variables are not significant, like Gender (p=0.4866) or Phone service (p=0.3388). Consequently, if a costumer is going to churn is independent of the gender and whether he has a phone service or not

- There are variables like "MultipleLines" that even if are significant (p=0.003464) the difference among levels is small as we can see in the plots

```{r, echo=FLASE}
profiling(df[8], df$Churn, "Churn")
```

- The rest of variables, including discretized variables, have an small p value (< 2.2e-16) and there is at least one level that has big differences of the proportions with the target. For example, people that do not have an online backup churned 40% while having the backup reduce the number to 21%.

```{r, echo=FLASE}
profiling(df[11], df$Churn, "Churn")
```

## Feature Selection
Lastly, we decided which variables were suitable to be included in the model. 

Initially, we removed the "id" as we consider that it will not give us any useful knowledge or be useful to predict the target. 

```{r}
# customerID should be removed
df$customerID <- NULL
```

Secondly, we computed the relationship of variables against the target:

-For categorical variables all p values are very low, less than 0.001, but the 6 variables with the lowest value are Contract, OnlineSecurity, TechSupport, c.tenure, InternetService, PaymentMethod. Note that the list includes one discrete variable.

```{r}
# Dind the correlation between the variables against our qualitative target Churn.
res.cat = catdes(df, 20)

# Most important categorical variables, sorted by p value
res.cat$test.chi2
```

- For the numeric variables "tenure" has the lowest p value, much lower than the discrete version. However, using one or the other will be decided in the modelling process. Additionally, in a previous section, we decided that the model will only include "MonthlyCharges" and "tenure" or "TotalCharges", as there is a high correlation among them. 

```{r}
res.cat$quanti.var
```

Lastly, we decided to make an extensive profiling for the six categorical variables that we could use in the model in order to understand them better. The main conclusions for each variable are:

-Contract: The probability to churn is decreased when the contract term increases. For example, if a costumer has a month contract and changes it to an annual the probability of not churning increases from 0.60 to 0.89.

-InternetService: People that do not have an internet service tend not to churn (92%). However, if they had a Fiber optic connection, the probability to churn gets higher (41%). This could be explained as users that use internet think that there are other companies than a better offer than the current one, but it would be necessary to make a market analysis to validate this hypothesis.

-OnlineSecurity: The probability to churn is reduced when the costumer has online security. However, having an internet connection seems a more interesting feature than the variable as "No internet service" level has a more important change of proportions than the other levels.

-TechSupport: Having a tech support increases the probability not to churn from 60% to 84%, but having internet or not is, again, a more relevant feature.

-c.tenure: Loyalty is important as people tend to churn less when they have stayed more months in the company. For example, if people have less than 1.5 they churned 44% of times, but with more than 4.5 years they churned only 0.7%.

-PaymentMethod: The proportion of people that churned is very similar in all the types of payment except for the electronic check. In this payment, the proportion to churn increases to 45% respect the 20% of other payments. 

```{r}
# Calculate the indexes of the variables to investigate
names = c("Contract", "OnlineSecurity", "TechSupport", "c.tenure", "InternetService", "PaymentMethod")
index = NULL

for (i in 1:length(names)) {
  ind = grep(names[i], colnames(df))
  print(ind)
  index = append(index, ind)
}

index = append(index, 21);

# Profiling of only those variables
res.cat2 = catdes(df[,index], 7)

res.cat2$category

# Another visualization of the profiling
#profiling(df[,index], df$Churn, "Churn")
```

# 6. Modeling 

## Data splitting
```{r}
# First, let's split the dataset into training and testing set. 
# We can consider that 70% of data will be used for training purpose.

set.seed(123)

sampling = sample.split(df$Churn, SplitRatio = 0.7)
train = subset(df, sampling == TRUE)
test = subset(df, sampling == FALSE)
```

## Modelling only with numerical variables.
```{r}
# As we mentioned, there is a strong correlation between {tenure, MonthlyCharges}
# and {TotalCharges}, as the second one is simply the product of first set.

m0.set1 = glm (Churn ~ tenure + MonthlyCharges, data = train, family = binomial)
# Checking the Anova test, both variable are significant to our model.
# Hence, we won't remove any of them.
Anova(m0.set1, test = "LR")

m0.set2 = glm (Churn ~ TotalCharges, data = train, family = binomial)

# Checking the Bayesian criterion, the set {tenure, MonthlyCharges} 
# has much lower value. Hence, we'll choose this for further analysis.
BIC(m0.set1, m0.set2)
```

```{r}
# Checking possible transformation for previous model

m0.log = glm (Churn ~ tenure + log(MonthlyCharges), data = train, family = binomial)
m0.sqrt = glm (Churn ~ sqrt(tenure) + MonthlyCharges, data = train, family = binomial)

# We have tried several transformations for both variable, but AIC shows that 
# the best model is the one with sqrt on the tenure.
BIC (m0.set1, m0.log, m0.sqrt)
```

```{r}
# Comparing categorical variables {c.tenure}. We'll like to make an comparison 
# between different kind of tenure (numerical and discretized)
m1 = glm (Churn ~ c.tenure + MonthlyCharges, data = train, family = binomial)

BIC(m1, m0.sqrt)
# Checking the AIC and BIC parameter, we decided to keep the tenure numerical, without discretization.
```

```{r, results= 'hide'}
# Check the influential plot
influent = influencePlot(m0.sqrt)[3]; influent

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh

# The cook's distance obtained from influence plot are smaller than
# the threshold, hence, we won't remove any point.
```

```{r}
# Adding categorical variables {Contract}
m2 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract, 
          data = train, family = binomial)

# Adding the {contract} to our model reduce the BIC.
BIC(m0.sqrt, m2)
```

```{r}
# Adding categorical variables  {InternetService}
m3 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService, 
          data = train, family = binomial)

BIC(m2,m3)
```

We have figured out in the profiling section that {InternetService} and {OnlineSecurity, TechSupport} have some level that are strongly correlated.
In the case that of "No Internet Service"

in common one level which is "No" internet service, the {OnlineSecurity} and 
{TechSupport} are also None.

Hence, to avoid NA's variables in the model, we've decided to keep
{InternetService} as this one is more correlated with target variable {Churn}
and the difference of BIC is not that significant.

```{r}
# Adding categorical variables {TechSupport} and {OnlineSecurity}
m4 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + OnlineSecurity 
          + TechSupport, data = train, family = binomial)

BIC(m3, m4)
```

```{r}
# Adding categorical variable {PaymentMethod}
m5 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod, data = train, family = binomial)

BIC(m3, m5)

summary(m5)
```

```{r, results= 'hide'}
# Check the influential plot before removing the influential observation.
influent = influencePlot(m5)[3]; influent

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh

# The cook's distance obtained from influence plot are smaller than
# the threshold, hence, we won't remove any point.
```

```{r}
# Check all the possible interactions of model m5.
m6 = glm (Churn ~ (sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod)^2, data = train, family = binomial)

step(m6)

```

                                 Df Deviance    AIC
<none>                                4022.7 4068.7
- sqrt(tenure):PaymentMethod      3   4031.4 4071.4
- sqrt(tenure):Contract           2   4033.0 4075.0
- MonthlyCharges:Contract         2   4035.2 4077.2
- Contract:InternetService        4   4042.4 4080.4
- MonthlyCharges:InternetService  2   4055.1 4097.1

```{r}
# Check the interaction between sqrt(tenure):PaymentMethod and
# sqrt(tenure):Contract
m7 = glm(Churn ~ sqrt(tenure) * PaymentMethod + sqrt(tenure) * Contract +
          MonthlyCharges + InternetService + PaymentMethod, data = train, 
          family = binomial)

BIC (m5, m7)
```

```{r}
# Check the interaction between MonthlyCharges and InternetService
m8 = glm(Churn ~ sqrt(tenure) +  Contract + MonthlyCharges * InternetService 
         + PaymentMethod, data = train, family = binomial)

BIC(m5,m7,m8)

summary(m8)
```

```{r}
# Check the effect of linked function
m9 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
          + PaymentMethod, data = train, family = binomial(link = "probit"))

# Based on BIC criterion, no improvement is obtained.
BIC(m5, m9)

```

```{r}
influencePlot(m9)

# Observation 269 and 4273 may be an influential point, but both of them
# are smaller than threshold.
```


```{r}
# Goodness of fit

# Residual plot

residualPlots(m5)


```


```{r}
# Model prediction. NOT WORKING NOW

predictions = predict(object = m5, newdata = test)

CM = table(test$Churn, predictions$class, dnn = c("Actual Churn", "Predicted Churn")); CM

accuracy = sum(diag(CM))/dim(dcon)[1]*100; accuracy

roc.curve(dcon$target, predictions$class)
```


```{r}
library(DescTools)
PseudoR2(m5, which = "McFadden")
```


