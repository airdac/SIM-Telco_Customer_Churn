TotalCharges.na <- which(is.na(df$TotalCharges))
sum(TotalCharges.na == which(df$tenure == 0)) == length(TotalCharges.na)
# So we transform them after creating a new numeric variable with all the missings of the database
df$n.na <- apply(sapply(df, is.na), 1, sum)
df$TotalCharges[TotalCharges.na] = 0
df$c.TotalCharges[TotalCharges.na] = "(-1,2171]"
# Chunk 13: Compare TotalCharges to its expected values
# Expected total charges as the product of monthly charges and tenure
expected_total_charges = df$MonthlyCharges * df$tenure
# Plot them against the actual total charges
plot(expected_total_charges, df$TotalCharges)
# There are no outliers, so TotalCharges is consistent.
# Chunk 14: Boxplots
par(mfrow=c(1, length(numeric_val_idx)))
for (var in as.numeric(numeric_val_idx)) {
Boxplot(df[,var], ylab = names(df)[var], main = "Boxplot")
}
par(mfrow=c(1,1))
# Chunk 15: Mean of missings for each group
# c.TotalCharges has missings, so it doesn't make sense to compute the mean of missings in its categories
interesting_cat_idx <- categoric_val_idx[-c(1,20)]
k = 0
for (i in interesting_cat_idx){
k <- k + length(levels(df[,i]))
}
groups.na <- matrix(0, k, 2)
l = 1
for (idx in interesting_cat_idx) {
categories.na <- tapply(df$n.na, df[,idx], mean)
for (j in seq(length(categories.na))) {
groups.na[l + j - 1,] <- c(categories.na[j],
paste(names(df)[idx], levels(df[,idx])[j],
sep = "."))
}
l <- l + j
}
groups.na.df <- data.frame(na.perc = groups.na[,1], group = groups.na[,2])
groups.na.df[order(groups.na.df$na.perc, decreasing = TRUE),]
# Chunk 16: n.na correlations
# Creation of the correlation matrix
corr_mat <- cor(df[,c(numeric_val_idx, 25)],)
corr_mat
corrplot(corr_mat, order = 'hclust', tl.cex = 0.9)
# Chunk 17: Remove n.na
# We remove n.na so that it does not interfere with the rest of the project
df$n.na <- NULL
# Chunk 18: Moutlier
set.seed(123)
res.mout <- Moutlier(df[,numeric_val_idx], quantile = 0.95, plot= FALSE)
# Visual representation
par(mfrow=c(1,2), cex.main=0.8)
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1), cex.main=1)
# Identification of the outliers
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff)
length(outliers)
length(outliers)/dim(df)[1]*100
# Chunk 19: Karinas profiling method
# Analysis of all variables except the ID
profiling(df[-c(grep("customerID", names(df)), grep("Churn", names(df)))], df$Churn, "Churn")
# Chunk 20: targets profiling with gender and PhoneService
profiling(df[c(grep("gender", names(df)),grep("PhoneService", names(df)))], df$Churn, "Churn")
# Chunk 21: targets profiling with MultipleLines
profiling(df[grep("MultipleLines", names(df))], df$Churn, "Churn")
# Chunk 22: targets profiling with OnlineBackup
profiling(df[grep("OnlineBackup", names(df))], df$Churn, "Churn")
# Chunk 23: Remove customerID
df$customerID <- NULL
# Chunk 24: catdes categorical
# Correlation between all variables and our qualitative target Churn.
res.cat = catdes(df, grep("Churn", names(df)))
# Most important categorical variables, sorted by p value
res.cat$test.chi2
# Chunk 25: catdes numerical
res.cat$quanti.var
# Chunk 26: Extensive profiling of most relevant categorical variables
# Global proportions of Churn categories
proportions(table(df$Churn))
# Calculate the indexes of the variables to investigate
names = c("Contract", "OnlineSecurity", "TechSupport", "c.tenure", "InternetService", "PaymentMethod")
index = NULL
for (i in 1:length(names)) {
ind = grep(names[i], colnames(df))
index = append(index, ind)
}
index = append(index, grep("Churn", names(df)))
# Profiling of only those variables
res.cat2 = catdes(df[,index], length(index))
res.cat2$category
# Another visualization of the profiling
# profiling(df[,index], df$Churn, "Churn")
# Chunk 27: Data splitting
set.seed(123)
sampling = sample.split(df$Churn, SplitRatio = 0.7)
train = subset(df, sampling == TRUE)
test = subset(df, sampling == FALSE)
# Chunk 28: Models with numeric variables
m0.set1 = glm (Churn ~ tenure + MonthlyCharges, data = train, family = binomial)
# Checking the Anova test, both variables are significant to our model. Hence, we won't remove any of them.
Anova(m0.set1, test = "LR")
m0.set2 = glm (Churn ~ TotalCharges, data = train, family = binomial)
BIC(m0.set1, m0.set2)
# Chunk 29: Transformations of numeric variables in the model
m0.log = glm (Churn ~ tenure + log(MonthlyCharges), data = train, family = binomial)
m0.sqrt = glm (Churn ~ sqrt(tenure) + MonthlyCharges, data = train, family = binomial)
BIC (m0.set1, m0.log, m0.sqrt)
# Chunk 30: Model with discretized numerical variables
m1 = glm (Churn ~ c.tenure + MonthlyCharges, data = train, family = binomial)
BIC(m1, m0.sqrt)
# Chunk 31: Influential data on model with numerical variables
# Check influential points
influent = influencePlot(m0.sqrt)[3]; influent
# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
# Chunk 32: Adding {Contract}
m2 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract,
data = train, family = binomial)
# Adding {contract} indeed reduces the BIC of our model.
BIC(m0.sqrt, m2)
# Chunk 33: Adding {InternetService}
m3 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService,
data = train, family = binomial)
# Adding {InternetService} indeed reduces the BIC of our model.
BIC(m2,m3)
# Chunk 34: Adding {TechSupport
m4 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + OnlineSecurity
+ TechSupport, data = train, family = binomial)
BIC(m3, m4)
# Chunk 35: Adding {PaymentMethod}
m5 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod, data = train, family = binomial)
# Adding {PaymentMethod} indeed reduces the BIC of our model.
BIC(m3, m5)
# Chunk 36: Influential data on model with categorical variables
influent = influencePlot(m5)[3]; influent
# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
# Chunk 37: Check all possible interactions of model m5
m6 = glm (Churn ~ (sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod)^2, data = train, family = binomial)
# Use step function to find the combination that minimizes the AIC.
step(m6)
# Chunk 38: sqrt(tenure):PaymentMethod and sqrt(tenure):Contract
m7 = glm(Churn ~ sqrt(tenure) * PaymentMethod + sqrt(tenure) * Contract +
MonthlyCharges + InternetService + PaymentMethod, data = train,
family = binomial)
BIC (m5, m7)
# Chunk 39: MonthlyCharges:InternetService
m8 = glm(Churn ~ sqrt(tenure) +  Contract + MonthlyCharges * InternetService
+ PaymentMethod, data = train, family = binomial)
BIC(m5,m8)
summary(m8)
# Chunk 40: probit
m9 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod, data = train, family = binomial(link = "probit"))
BIC(m5, m9)
# Chunk 41: Final Influential data analysis
# Check influential points
influent = influencePlot(m9)[3]; influent
# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
# The most influential observations are the 269 and 4273, which are the ones with the biggest Cook's distance. Nonetheless, any of them is a multivariate outlier.
sum(outliers==269)
sum(outliers==4273)
df$Churn[269]
df$Churn[4273]
# Neither is a univariate outlier in tenure and TotalCharges when analyzed inside their target's category.
sum(Boxplot(df$tenure[df$Churn=="Yes"])==269)
sum(Boxplot(df$tenure[df$Churn=="Yes"])==4273)
sum(Boxplot(df$MonthlyCharges[df$Churn=="Yes"])==269)
sum(Boxplot(df$MonthlyCharges[df$Churn=="Yes"])==4273)
sum(Boxplot(df$TotalCharges[df$Churn=="Yes"])==269)
sum(Boxplot(df$TotalCharges[df$Churn=="Yes"])==4273)
# Chunk 42: PseudoR2()
PseudoR2(m5, which = "McFadden")
# Chunk 43: Residual plots
residualPlots(m5)
# Outliers in the residual plots might be caused, in part, by unbalanced data
prop.table(table(df$Contract))
prop.table(table(df$InternetService))
prop.table(table(df$PaymentMethod))
# Chunk 44: m5.out
m5.mout = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod, data = train[-outliers,], family = binomial)
residualPlots(m5.mout)
# Chunk 45: predict()
# First, we compute the probability of Churn for each observation (from test) with predict function.
predictions = predict(m5, test[-20], type = "response")
# Then, for those that have a probability higher than 0.5, we can consider Churn == "Yes"
probability = factor(as.character(ifelse(predictions >= 0.5, "Yes", "No")))
# Finally, compute the Confusion Matrix of predicted result
confusionMatrix(probability, test$Churn, mode = "everything", positive="Yes")
roc.curve(test$Churn, probability)
# Chunk 46
m.null = glm(Churn ~ 1,  data = train, family = binomial)
BIC (m5, m.null)
# Chunk 47
m5 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod, data = train, family = binomial)
summary(m5)
sort(exp(m5$coefficients), decreasing = TRUE)
# Analysis of all variables except the ID
profiling(df[-c(grep("customerID", names(df)), grep("Churn", names(df)))], df$Churn, "Churn")
rmarkdown::render("main.Rmd")
rmarkdown::render("main.Rmd")
# First, we compute the probability of Churn for each observation (from test) with predict function.
predictions = predict(m5, test[-20], type = "response")
# Then, for those that have a probability higher than 0.5, we can consider Churn == "Yes"
probability = factor(as.character(ifelse(predictions >= 0.5, "Yes", "No")))
# Finally, compute the Confusion Matrix of predicted result
confusion.mat <- confusionMatrix(probability, test$Churn, mode = "everything", positive="Yes"); confusion.mat
roc.curve(test$Churn, probability)
names(confusion.mat)
confusion.mat$table
rmarkdown::render("main.Rmd")
rmarkdown::render("main.Rmd")
confusion.mat$table
rmarkdown::render("main.Rmd")
rmarkdown::render("main.Rmd")
rmarkdown::render("main.Rmd")
rmarkdown::render("main.Rmd")
# Chunk 1
library(knitr)
opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# Chunk 2: Clean workspace. Load libraries
if(!is.null(dev.list())) dev.off()
rm(list = ls())
library(dplyr)
library(car)
library(DataExplorer)
library(FactoMineR)
library(caTools)
library(chemometrics)
library(corrplot)
source("profiling_func.R")
library(ROSE)
library(DescTools)
library(caret)
library(tidyverse)
library(DT)
library(kableExtra)
# Chunk 3: Import dataset
df = read.csv("WA_Fn-UseC_-Telco-Customer-Churn.xls",header=T, sep=",",
stringsAsFactors=TRUE)
# Chunk 4: First look at df
head(df)
dim(df)
summary(df)
# Chunk 5: SeniorCitizen to categorical
df$SeniorCitizen <- factor(df$SeniorCitizen, labels = c("Yes", "No"))
# Chunk 6: Discretize tenure
df$c.tenure <- df$tenure # Create a new variable called Categorical.tenure
m.tenure <- max(df$tenure, na.rm = TRUE)
df$c.tenure <- replace(df$c.tenure, df$tenure <= m.tenure/4, m.tenure/4)
for (i in 1:3) {
idx <- (m.tenure*i/4 < df$tenure) & (df$tenure <= m.tenure*(i+1)/4)
df$c.tenure <- replace(df$c.tenure, idx, m.tenure*(i+1)/4)
}
min(df$tenure, na.rm = TRUE)
breakpts <- seq(m.tenure/4, m.tenure, m.tenure/4); breakpts
df$c.tenure <- factor(df$c.tenure, labels = c("(-1,18]", "(18,36]",
"(36,54]", "(54,72]"))
summary(df$c.tenure)
par(mfrow=c(1,2))
plot(df$c.tenure, main = "Barplot of df$c.tenure")
hist(df$tenure)
# Chunk 7: Discretize TotalCharges
df$c.TotalCharges <- df$TotalCharges
m.TotalCharges <- max(df$TotalCharges, na.rm = TRUE)
df$c.TotalCharges <- replace(df$c.TotalCharges, df$TotalCharges <= m.TotalCharges/4, m.TotalCharges/4)
for (i in 1:3) {
idx <- (m.TotalCharges*i/4 < df$TotalCharges) & (df$TotalCharges <=
m.TotalCharges*(i+1)/4)
df$c.TotalCharges <- replace(df$c.TotalCharges, idx, m.TotalCharges*(i+1)/4)
}
breakpts <- seq(m.TotalCharges/4, m.TotalCharges, m.TotalCharges/4); breakpts
df$c.TotalCharges <- factor(df$c.TotalCharges, labels = c("(-1,2171]",
"(2171,4342]",
"(4342,6514]",
"(6514,8685]"))
summary(df$c.TotalCharges)
par(mfrow=c(1,2))
plot(df$c.TotalCharges, main = "Barplot of df$c.TotalCharges")
hist(df$TotalCharges)
# Chunk 8: Discretize MonthlyCharges
df$c.MonthlyCharges <- df$MonthlyCharges
m.MonthlyCharges <- max(df$MonthlyCharges, na.rm = TRUE)
df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, df$MonthlyCharges <= m.MonthlyCharges/4, m.MonthlyCharges/4)
for (i in 1:3) {
idx <- (m.MonthlyCharges*i/4 < df$MonthlyCharges) & (df$MonthlyCharges <=
m.MonthlyCharges*(i+1)/4)
df$c.MonthlyCharges <- replace(df$c.MonthlyCharges, idx,
m.MonthlyCharges*(i+1)/4)
}
min(df$MonthlyCharges, na.rm = TRUE)
breakpts <- seq(m.MonthlyCharges/4, m.MonthlyCharges, m.MonthlyCharges/4)
breakpts
df$c.MonthlyCharges <- factor(df$c.MonthlyCharges, labels = c("(18,30.69]",
"(30.69,59.38]",
"(59.38,89.06]",
"(89.06,118.75]"))
summary(df$c.MonthlyCharges)
par(mfrow=c(1,2))
plot(df$c.MonthlyCharges, main = "Barplot of df$c.MonthlyCharges")
hist(df$MonthlyCharges)
par(mfrow=c(1,1))
# Chunk 9: Identify categorical and numerical variables
numeric_val_idx = which(sapply(df, is.numeric))
numeric_val = names(df)[numeric_val_idx]
# The only numerical features that we have are tenure, MonthlyCharges and TotalChages.
# So the remaining will be categorical features.
categoric_val_idx = which(sapply(df, is.factor))
categoric_val = names(df)[categoric_val_idx]
# Chunk 10: EDA
# Basic EDA
summary(df)
# Complete EDA
#create_report(df, output_format = "pdf_document", output_file = "Telco.pdf")
# Chunk 11: Analysis of normality
# tests
ks.test(df$TotalCharges, "pnorm")
ks.test(df$MonthlyCharges, "pnorm")
ks.test(df$tenure, "pnorm")
# plots
par(mfrow=c(1,2))
hist(df$tenure, prob = TRUE, breaks = 10, main = 'Histogram of tenure
vs normal distribution', xlab = 'tenure')
x <- seq(min(df$tenure), max(df$tenure), by = .1)
y <- dnorm(x, mean = mean(df$tenure), sd = sd(df$tenure))
plot(x,y, xlab = 'tenuere', ylab = '')
hist(df$TotalCharges, prob = TRUE, breaks = 10, main = 'Hist totalCharges
vs normal distribution', xlab = 'TotalCharges')
x <- seq(min(df$TotalCharges, na.rm = TRUE), max(df$TotalCharges, na.rm = TRUE),
by = 10)
y <- dnorm(x, mean = mean(df$TotalCharges, na.rm = TRUE), sd = sd(df$TotalCharges, na.rm = TRUE))
plot(x,y, xlab = 'TotalCharges', ylab = '')
hist(df$MonthlyCharges, prob = TRUE, breaks = 10, main = 'Hist MonthlyCharges
vs normal distribution', xlab = 'df$MonthlyCharges')
x <- seq(min(df$MonthlyCharges, na.rm = TRUE), max(df$MonthlyCharges, na.rm = TRUE),
by = .1)
y <- dnorm(x, mean = mean(df$MonthlyCharges, na.rm = TRUE), sd = sd(df$MonthlyCharges, na.rm = TRUE))
plot(x,y, xlab = 'df$MonthlyCharges', ylab = '')
par(mfrow=c(1,1))
# Chunk 12: Distribution of missings
# Distribution of missings in df per variable
apply(sapply(df, is.na), 2, sum)
# Distribution of missings in df per individual
table(apply(sapply(df, is.na), 1, sum))
# Check that all missings in "TotalCharges" correspond to individuals tenure = 0
TotalCharges.na <- which(is.na(df$TotalCharges))
sum(TotalCharges.na == which(df$tenure == 0)) == length(TotalCharges.na)
# So we transform them after creating a new numeric variable with all the missings of the database
df$n.na <- apply(sapply(df, is.na), 1, sum)
df$TotalCharges[TotalCharges.na] = 0
df$c.TotalCharges[TotalCharges.na] = "(-1,2171]"
# Chunk 13: Compare TotalCharges to its expected values
# Expected total charges as the product of monthly charges and tenure
expected_total_charges = df$MonthlyCharges * df$tenure
# Plot them against the actual total charges
plot(expected_total_charges, df$TotalCharges)
# There are no outliers, so TotalCharges is consistent.
# Chunk 14: Boxplots
par(mfrow=c(1, length(numeric_val_idx)))
for (var in as.numeric(numeric_val_idx)) {
Boxplot(df[,var], ylab = names(df)[var], main = "Boxplot")
}
par(mfrow=c(1,1))
# Chunk 15: Mean of missings for each group
# c.TotalCharges has missings, so it doesn't make sense to compute the mean of missings in its categories
interesting_cat_idx <- categoric_val_idx[-c(1,20)]
k = 0
for (i in interesting_cat_idx){
k <- k + length(levels(df[,i]))
}
groups.na <- matrix(0, k, 2)
l = 1
for (idx in interesting_cat_idx) {
categories.na <- tapply(df$n.na, df[,idx], mean)
for (j in seq(length(categories.na))) {
groups.na[l + j - 1,] <- c(categories.na[j],
paste(names(df)[idx], levels(df[,idx])[j],
sep = "."))
}
l <- l + j
}
groups.na.df <- data.frame(na.perc = groups.na[,1], group = groups.na[,2])
groups.na.df[order(groups.na.df$na.perc, decreasing = TRUE),]
# Chunk 16: n.na correlations
# Creation of the correlation matrix
corr_mat <- cor(df[,c(numeric_val_idx, 25)],)
corr_mat
corrplot(corr_mat, order = 'hclust', tl.cex = 0.9)
# Chunk 17: Remove n.na
# We remove n.na so that it does not interfere with the rest of the project
df$n.na <- NULL
# Chunk 18: Moutlier
set.seed(123)
res.mout <- Moutlier(df[,numeric_val_idx], quantile = 0.95, plot= FALSE)
# Visual representation
par(mfrow=c(1,2), cex.main=0.8)
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1), cex.main=1)
# Identification of the outliers
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff)
length(outliers)
length(outliers)/dim(df)[1]*100
# Chunk 19: Karinas profiling method
# Analysis of all variables except the ID
profiling(df[-c(grep("customerID", names(df)), grep("Churn", names(df)))], df$Churn, "Churn")
# Chunk 20: targets profiling with gender and PhoneService
profiling(df[c(grep("gender", names(df)),grep("PhoneService", names(df)))], df$Churn, "Churn")
# Chunk 21: targets profiling with MultipleLines
profiling(df[grep("MultipleLines", names(df))], df$Churn, "Churn")
# Chunk 22: targets profiling with OnlineBackup
profiling(df[grep("OnlineBackup", names(df))], df$Churn, "Churn")
# Chunk 23: Remove customerID
df$customerID <- NULL
# Chunk 24: catdes categorical
# Correlation between all variables and our qualitative target Churn.
res.cat = catdes(df, grep("Churn", names(df)))
# Most important categorical variables, sorted by p value
res.cat$test.chi2
# Chunk 25: catdes numerical
res.cat$quanti.var
# Chunk 26: Extensive profiling of most relevant categorical variables
# Global proportions of Churn categories
proportions(table(df$Churn))
# Calculate the indexes of the variables to investigate
names = c("Contract", "OnlineSecurity", "TechSupport", "c.tenure", "InternetService", "PaymentMethod")
index = NULL
for (i in 1:length(names)) {
ind = grep(names[i], colnames(df))
index = append(index, ind)
}
index = append(index, grep("Churn", names(df)))
# Profiling of only those variables
res.cat2 = catdes(df[,index], length(index))
res.cat2$category
# Another visualization of the profiling
# profiling(df[,index], df$Churn, "Churn")
# Chunk 27: Data splitting
set.seed(123)
sampling = sample.split(df$Churn, SplitRatio = 0.7)
train = subset(df, sampling == TRUE)
test = subset(df, sampling == FALSE)
# Chunk 28: Models with numeric variables
m0.set1 = glm (Churn ~ tenure + MonthlyCharges, data = train, family = binomial)
# Checking the Anova test, both variables are significant to our model. Hence, we won't remove any of them.
Anova(m0.set1, test = "LR")
m0.set2 = glm (Churn ~ TotalCharges, data = train, family = binomial)
BIC(m0.set1, m0.set2)
# Chunk 29: Transformations of numeric variables in the model
m0.log = glm (Churn ~ tenure + log(MonthlyCharges), data = train, family = binomial)
m0.sqrt = glm (Churn ~ sqrt(tenure) + MonthlyCharges, data = train, family = binomial)
BIC (m0.set1, m0.log, m0.sqrt)
# Chunk 30: Model with discretized numerical variables
m1 = glm (Churn ~ c.tenure + MonthlyCharges, data = train, family = binomial)
BIC(m1, m0.sqrt)
# Chunk 31: Influential data on model with numerical variables
# Check influential points
influent = influencePlot(m0.sqrt)[3]; influent
# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
# Chunk 32: Adding {Contract}
m2 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract,
data = train, family = binomial)
# Adding {contract} indeed reduces the BIC of our model.
BIC(m0.sqrt, m2)
# Chunk 33: Adding {InternetService}
m3 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService,
data = train, family = binomial)
# Adding {InternetService} indeed reduces the BIC of our model.
BIC(m2,m3)
# Chunk 34: Adding {TechSupport
m4 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + OnlineSecurity
+ TechSupport, data = train, family = binomial)
BIC(m3, m4)
# Chunk 35: Adding {PaymentMethod}
m5 = glm (Churn ~ sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod, data = train, family = binomial)
# Adding {PaymentMethod} indeed reduces the BIC of our model.
BIC(m3, m5)
# Chunk 36: Influential data on model with categorical variables
influent = influencePlot(m5)[3]; influent
# Calculate D's threshold
D_thresh <- 2/sqrt(dim(train)[1]); D_thresh
# Chunk 37: Check all possible interactions of model m5
m6 = glm (Churn ~ (sqrt(tenure) + MonthlyCharges + Contract + InternetService
+ PaymentMethod)^2, data = train, family = binomial)
# Use step function to find the combination that minimizes the AIC.
step(m6)
rmarkdown::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
knitr::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
rmarkdown::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
confusion.mat$table
rmarkdown::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
rmarkdown::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
confusion.mat$table
confusion.mat$table
rmarkdown::render("AdriàCasanova-VíctorGarcia-ZhengyongJi.Rmd")
